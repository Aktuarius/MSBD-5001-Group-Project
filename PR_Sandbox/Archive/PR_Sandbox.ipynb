{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5370d3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use PyTorch to build neural networkabs\n",
    "# Inspired by pytorch workshop conducted in MSBD 5001 course\n",
    "\n",
    "# Import required packages\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToPILImage\n",
    "from torch.nn.init import kaiming_uniform_\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3ea00b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the yield data as pandas data frame (Note this is saved as a csv file in base working directory)\n",
    "file_path = os.path.join(os.getcwd()) + '/'\n",
    "file_name = 'Crop_Yields.csv'\n",
    "yield_data = pd.read_csv(file_path + file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39f2b632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create class to load dataset\n",
    "\n",
    "class CustomImageDataset():\n",
    "    def __init__(self, hist_dir, crop_yield_table, transform=None, target_transform=None):  \n",
    "        self.hist_dir = hist_dir # References the directory for each given country where the data is stored\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        # load the files \n",
    "        filenames = [] # For every year, yields a numpy array of dimension 576 x 29\n",
    "        crop_yields = []\n",
    "        for filename in os.listdir(hist_dir):\n",
    "            filenames.append(filename)\n",
    "            crop_yields.append(crop_yield_table[crop_yield_table.Year == int(filename[-8:-4])]['Cereal Yield'].values[0])\n",
    "        self.filenames = filenames\n",
    "        self.crop_yields =crop_yields\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        hist_dir = os.path.join(self.hist_dir, self.filenames[idx])\n",
    "        #hist = torch.from_numpy(np.load(hist_dir)).float()\n",
    "        hist = np.load(hist_dir)\n",
    "        if self.transform:\n",
    "            hist = self.transform(hist).float()\n",
    "        crop_yield = torch.from_numpy(self.crop_yields[idx].reshape(-1, 1)).float()\n",
    "        return hist, crop_yield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2d7e460",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Resize, ToTensor, Compose\n",
    "\n",
    "# crop_dataset = CustomImageDataset(\"./kenya\", crop_yield_table = yield_data, transform = ToTensor())\n",
    "crop_dataset = CustomImageDataset(\"./kenya\", crop_yield_table = yield_data, transform = Compose([ToTensor(), Resize((28,28))]))\n",
    "\n",
    "# print(dataset.filenames)\n",
    "# print(dataset.crop_yields)\n",
    "# dataset[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b3556c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[9.2558e-03, 1.3343e-03, 3.7762e-04, 3.8769e-03, 3.7510e-03,\n",
       "           1.2839e-02, 0.0000e+00, 3.7762e-04, 9.8181e-04, 0.0000e+00,\n",
       "           0.0000e+00, 2.7021e-03, 3.6419e-03, 4.9846e-03, 7.6615e-03,\n",
       "           5.1692e-03, 2.9538e-03, 6.4027e-03, 3.1888e-03, 0.0000e+00,\n",
       "           2.0643e-03, 4.6237e-03, 1.0154e-03, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.3804e-02, 4.7227e-02, 1.0162e-02, 1.8914e-02, 6.5555e-02,\n",
       "           1.1936e-01, 1.6118e-01, 1.8856e-02, 9.7333e-02, 3.0745e-01,\n",
       "           3.0592e-01, 1.5748e-01, 8.8933e-02, 6.9373e-02, 2.6786e-02,\n",
       "           7.0430e-02, 1.0509e-01, 2.1258e-01, 1.4697e-01, 1.5784e-01,\n",
       "           1.0168e-01, 4.3820e-02, 2.2968e-02, 2.8019e-02, 1.8092e-02,\n",
       "           2.8430e-02, 1.7152e-02, 2.9370e-04],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [9.5915e-02, 2.0014e-02, 1.9720e-03, 2.1910e-02, 2.5359e-02,\n",
       "           4.1043e-02, 3.9692e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           1.9384e-03, 1.7714e-02, 2.2078e-02, 6.5286e-03, 4.5818e-03,\n",
       "           5.8321e-03, 3.8601e-03, 1.7916e-02, 1.7488e-02, 9.1300e-03,\n",
       "           1.5558e-02, 1.1471e-02, 1.8461e-03, 0.0000e+00, 0.0000e+00,\n",
       "           8.5593e-03, 5.0349e-04, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.9740e-01, 3.0339e-01, 3.2306e-01, 3.8369e-01, 3.4010e-01,\n",
       "           2.5772e-01, 3.3739e-01, 6.4350e-01, 5.7141e-01, 1.8135e-01,\n",
       "           2.9223e-01, 5.2636e-01, 5.2254e-01, 3.8975e-01, 3.4278e-01,\n",
       "           4.5031e-01, 4.9312e-01, 3.6184e-01, 3.5165e-01, 2.4065e-01,\n",
       "           2.9839e-01, 4.7580e-01, 3.2783e-01, 5.2096e-01, 3.6196e-01,\n",
       "           5.3471e-01, 5.7052e-01, 8.5666e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 4.1958e-03, 4.8671e-03, 0.0000e+00,\n",
       "           5.3434e-03, 3.9644e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [5.0769e-03, 0.0000e+00, 0.0000e+00, 1.2923e-03, 1.0338e-02,\n",
       "           8.3076e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 5.1882e-03, 1.3272e-03, 4.3671e-03, 6.5043e-04,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 8.2237e-04, 6.5034e-03,\n",
       "           4.9090e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.4335e-04,\n",
       "           2.0979e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 2.9370e-04, 1.9720e-03,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00]]]),\n",
       " tensor([[1.7400]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crop_dataset[3] # Shows for a given tensor which consists of a pair of 1) array of binned histogram data; and 2) yield output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a188034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crop_dataset[3][0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "200eb59b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crop_dataset[3][1].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4825edc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crop_dataset[3][0].shape # For each binned histogram data, consists of 576 observations across 29 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d030486b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of the image:  <class 'torch.Tensor'>\n",
      "Yield: tensor([[1.2400]])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1332x756 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot data for one sample pair\n",
    "raw_image, crop_yield = crop_dataset[0]\n",
    "figure = plt.figure()\n",
    "figure.set_size_inches(18.5, 10.5)\n",
    "\n",
    "print(\"type of the image: \", type(raw_image))\n",
    "\n",
    "if type(raw_image) == torch.Tensor:\n",
    "    plt.imshow(ToPILImage()(raw_image))\n",
    "else:\n",
    "    plt.imshow(raw_image)\n",
    "    \n",
    "print(f\"Yield: {crop_yield}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f900a035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22a4fe1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now moving on to the machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a733f685",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(crop_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "235b65d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([1, 1, 28, 28])\n",
      "tensor([[[1.2400]]])\n",
      "Feature data type: torch.float32\n",
      "Label data type: torch.float32\n",
      "Feature batch shape: torch.Size([1, 1, 28, 28])\n",
      "tensor([[[1.7100]]])\n",
      "Feature data type: torch.float32\n",
      "Label data type: torch.float32\n",
      "Feature batch shape: torch.Size([1, 1, 28, 28])\n",
      "tensor([[[1.5100]]])\n",
      "Feature data type: torch.float32\n",
      "Label data type: torch.float32\n",
      "Feature batch shape: torch.Size([1, 1, 28, 28])\n",
      "tensor([[[1.7400]]])\n",
      "Feature data type: torch.float32\n",
      "Label data type: torch.float32\n",
      "Feature batch shape: torch.Size([1, 1, 28, 28])\n",
      "tensor([[[1.6600]]])\n",
      "Feature data type: torch.float32\n",
      "Label data type: torch.float32\n",
      "Feature batch shape: torch.Size([1, 1, 28, 28])\n",
      "tensor([[[1.5800]]])\n",
      "Feature data type: torch.float32\n",
      "Label data type: torch.float32\n",
      "Feature batch shape: torch.Size([1, 1, 28, 28])\n",
      "tensor([[[1.7700]]])\n",
      "Feature data type: torch.float32\n",
      "Label data type: torch.float32\n",
      "Feature batch shape: torch.Size([1, 1, 28, 28])\n",
      "tensor([[[1.3800]]])\n",
      "Feature data type: torch.float32\n",
      "Label data type: torch.float32\n",
      "Feature batch shape: torch.Size([1, 1, 28, 28])\n",
      "tensor([[[1.6300]]])\n",
      "Feature data type: torch.float32\n",
      "Label data type: torch.float32\n",
      "Feature batch shape: torch.Size([1, 1, 28, 28])\n",
      "tensor([[[1.8100]]])\n",
      "Feature data type: torch.float32\n",
      "Label data type: torch.float32\n"
     ]
    }
   ],
   "source": [
    "for feature, label in dataloader:\n",
    "    print(f\"Feature batch shape: {feature.size()}\")\n",
    "    print(label)\n",
    "    print(f\"Feature data type: {feature.dtype}\")\n",
    "    print(f\"Label data type: {label.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8ef163a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Check device used\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24845229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is Neural Network as outlined in lecture notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69ecd505",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        #self.linear1 = nn.Linear(28*28, 512)\n",
    "        #self.linear2 = nn.Linear(512, 512)\n",
    "        #self.linear3 = nn.Linear(512, 10)\n",
    "        #self.relu = nn.ReLU()\n",
    "        \n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        #x = self.linear1(x)\n",
    "        #x = self.relu(x)\n",
    "        #x = self.linear2(x)\n",
    "        #x = self.relu(x)\n",
    "        #logits = self.linear3(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13af5a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "ConvNN(\n",
      "  (conv_stack): Sequential(\n",
      "    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Flatten(start_dim=1, end_dim=-1)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=192, out_features=512, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "    \n",
    "# Define model\n",
    "class ConvNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNN, self).__init__()\n",
    "        self.conv_stack = nn.Sequential(\n",
    "            nn.Conv2d(3,6,5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(6,12,5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(192, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 3)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.conv_stack(x)\n",
    "        return logits\n",
    "    \n",
    "\n",
    "conv_model = ConvNN().to(device)\n",
    "print(conv_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a01f3ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9c30979",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6440/3325940514.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcrop_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6440/2502682347.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;31m#x = self.relu(x)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;31m#logits = self.linear3(x)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_relu_stack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_addmm)"
     ]
    }
   ],
   "source": [
    "print(model(crop_dataset[3][0].to(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ba6cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = dataloader # train_dataloader_resnet\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=1e-4)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69f98a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.train()\n",
    "    correct, train_loss = 0, 0 \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        train_loss += loss.item()\n",
    "        print(f\"Actual {y}\")\n",
    "        print(f\"Predicted {pred}\")\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()  \n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "    train_loss /= num_batches\n",
    "    return train_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8b27d90",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6440/2857850490.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;31m# test_loss, test_acc = test(testloader, model, loss_fn)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trainloader' is not defined"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "for t in range(epochs):\n",
    "    train_loss = train(trainloader, model, loss_fn, optimizer)\n",
    "    # test_loss, test_acc = test(testloader, model, loss_fn)\n",
    "    if t%1 == 0:\n",
    "        print(f\"Epoch {t}:\")\n",
    "        print(f\"Avg loss: {train_loss:>8f}\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea370f8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49623cf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932a126f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00575dcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
