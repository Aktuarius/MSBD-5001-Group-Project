{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "c713128a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imread\n",
    "from matplotlib.pyplot import imshow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.imagenet_utils import decode_predictions\n",
    "from tensorflow.keras.applications.imagenet_utils import preprocess_input\n",
    "from tensorflow.keras import layers, models, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.losses import MeanAbsoluteError, MeanAbsolutePercentageError\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import History\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "fcd46b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import seaborn as sns\n",
    "from typing import Iterator, List, Union, Tuple\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "88a4a4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "59fc663d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_water_data() -> pd.DataFrame:\n",
    "    countries_list_nme = get_selected_africa_coutries_list()\n",
    "    processed_path = Path(str(Path(os.getcwd()).parent.absolute()) + '/WaterProcessed')\n",
    "    country_list = sorted(processed_path.glob('*'))\n",
    "    data = []\n",
    "    for i in country_list:\n",
    "        f = open(str(i))\n",
    "        water_data = json.load(f)\n",
    "        country_name =  str(i).split('\\\\')[-1].split('.')[-2][:-5].replace('_', ' ')\n",
    "        water_data['country_nme'] = country_name\n",
    "        if country_name in countries_list_nme:\n",
    "            data.append(water_data)\n",
    "    water_df = pd.DataFrame(data=data)\n",
    "    water_df = water_df.set_index('country_nme')\n",
    "    return water_df\n",
    "\n",
    "def get_ndvi_evi_data() -> [pd.DataFrame, pd.DataFrame]:\n",
    "    countries_list_nme = get_selected_africa_coutries_list()\n",
    "    processed_path = Path(str(Path(os.getcwd()).parent.absolute()) + '/NDVI/Processed_edvi_data')\n",
    "    country_list = sorted(processed_path.glob('*'))\n",
    "    ndvi = []\n",
    "    evi = []\n",
    "    ndvi_index_list = None\n",
    "    evi_index_list = None\n",
    "    for country in country_list:\n",
    "        data_list = sorted(country.glob('*.csv'))\n",
    "        country_nme = str(country).split('\\\\')[-1]\n",
    "        country_nme = country_nme.replace('_', ' ')\n",
    "        if country_nme not in countries_list_nme:\n",
    "            continue\n",
    "        for i in data_list:\n",
    "            file_name = str(i).split('\\\\')[-1].split('_')[0]\n",
    "            temp_data = pd.read_csv(str(i), index_col = 0)\n",
    "            temp_data.columns = [country_nme]\n",
    "\n",
    "            if file_name == 'NDVI':\n",
    "                if ndvi_index_list is None:\n",
    "                    ndvi_index_list = temp_data.index.tolist()\n",
    "                temp_data = temp_data.reset_index(drop=True)\n",
    "                ndvi.append(temp_data)\n",
    "            else:\n",
    "                if evi_index_list is None:\n",
    "                    evi_index_list = temp_data.index.tolist()\n",
    "                temp_data = temp_data.reset_index(drop=True)\n",
    "                evi.append(temp_data)\n",
    "    ndvi_df = pd.concat(ndvi, axis=1)\n",
    "    evi_df = pd.concat(evi, axis=1)\n",
    "    ndvi_df.index = ndvi_index_list\n",
    "    evi_df.index = evi_index_list\n",
    "    return ndvi_df, evi_df\n",
    "\n",
    "\n",
    "def ndvi_evi_monthly_to_yearly(ndvi, evi):\n",
    "    ndvi = ndvi.reset_index()\n",
    "    evi = evi.reset_index()\n",
    "\n",
    "    ndvi['year'] = ndvi['index'].apply(lambda x: x[:4])\n",
    "    evi['year'] = evi['index'].apply(lambda x: x[:4])\n",
    "\n",
    "    ndvi_mean_df = ndvi.groupby(['year']).mean()\n",
    "    ndvi_max_df = ndvi.groupby(['year']).max()\n",
    "    ndvi_min_df = ndvi.groupby(['year']).min()\n",
    "\n",
    "    evi_mean_df = evi.groupby(['year']).mean()\n",
    "    evi_max_df = evi.groupby(['year']).max()\n",
    "    evi_min_df = evi.groupby(['year']).min()\n",
    "\n",
    "    # ndvi_mean_df = ndvi_mean_df.drop(columns=['index'])\n",
    "    ndvi_max_df = ndvi_max_df.drop(columns=['index'])\n",
    "    ndvi_min_df = ndvi_min_df.drop(columns=['index'])\n",
    "\n",
    "    evi_max_df = evi_max_df.drop(columns=['index'])\n",
    "    evi_min_df = evi_min_df.drop(columns=['index'])\n",
    "\n",
    "    ndvi_mean_df = ndvi_mean_df.T\n",
    "    ndvi_max_df = ndvi_max_df.T\n",
    "    ndvi_min_df = ndvi_min_df.T\n",
    "    evi_mean_df = evi_mean_df.T\n",
    "    evi_max_df = evi_max_df.T\n",
    "    evi_min_df = evi_min_df.T\n",
    "\n",
    "    return ndvi_mean_df, ndvi_max_df, ndvi_min_df, evi_mean_df, evi_max_df, evi_min_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fbe8772a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_selected_africa_coutries_list() -> list:\n",
    "    processed_path = Path(str(Path(os.getcwd()).parent.absolute()) + '/PR_Sandbox/data_country_folders')\n",
    "    country_list = sorted(processed_path.glob('*'))\n",
    "    return [str(country).split('\\\\')[-1].replace('_', ' ') for country in country_list]\n",
    "\n",
    "def get_label_data() -> [pd.DataFrame, MinMaxScaler] :\n",
    "    label_path = str(Path(os.getcwd()).parent.absolute()) + '/Yield_Data/all_country_crop_yield_tons_per_hectare1.csv'\n",
    "    df = pd.read_csv(label_path)\n",
    "    df = df.set_index(['Country Name'])\n",
    "    df = df.iloc[:, :-3]\n",
    "    countries_list_nme = get_selected_africa_coutries_list()\n",
    "    temp2 = [i in countries_list_nme for i in df.index.tolist()]\n",
    "    df2 = df[temp2]\n",
    "    print('Data Range Before Scale:',df2.max().max(), ' to ',  df2.min().min())\n",
    "    scaler = MinMaxScaler(feature_range=(0, 2))\n",
    "    temp = df2.copy().to_numpy().reshape(-1, 1)\n",
    "    scaler = scaler.fit(temp)\n",
    "    for i in df2.columns.tolist():\n",
    "        df2.loc[:, i] = scaler.transform(df2[i].values.reshape(-1, 1))\n",
    "    print('Data Range After Scale:', df2.max().max(), ' to ', df2.min().min())\n",
    "    return df2, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4c1b60c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_extraction_and_batching(df_label:pd.DataFrame) -> [list, list, list, list, list, pd.DataFrame, pd.DataFrame]:\n",
    "    processed_path = Path(str(Path(os.getcwd()).parent.absolute()) + '/PR_Sandbox/data_country_folders')\n",
    "    country_list = sorted(processed_path.glob('*'))\n",
    "    train_x, train_y, test_x, valid_x, valid_y = [], [], [], [], []\n",
    "    water_train_x, water_test_x = [], []\n",
    "    ndvi_mean_train_x, ndvi_mean_test_x = [], []\n",
    "    ndvi_max_train_x, ndvi_max_test_x = [], []\n",
    "    ndvi_min_train_x, ndvi_min_test_x = [], []\n",
    "    evi_mean_train_x, evi_mean_test_x = [], []\n",
    "    evi_max_train_x, evi_max_test_x = [], []\n",
    "    evi_min_train_x, evi_min_test_x = [], []\n",
    "    year_train_x, year_test_x = [], []\n",
    "    country_train_x, country_test_x =[],[]\n",
    "\n",
    "#     counter, sum,counter_new, sum_new= 0, 0, 0, 0\n",
    "\n",
    "    water_data = get_water_data()\n",
    "    ndvi, evi =  get_ndvi_evi_data()\n",
    "    ndvi_mean_df, ndvi_max_df, ndvi_min_df, evi_mean_df, evi_max_df, evi_min_df = ndvi_evi_monthly_to_yearly(ndvi, evi)\n",
    "\n",
    "    scaler_data = None\n",
    "    for country in country_list:\n",
    "        if '.' in str(country):\n",
    "            continue\n",
    "        data_list = sorted(country.glob('*.npy'))\n",
    "        country_nme = str(country).split('\\\\')[-1]\n",
    "        country_nme = country_nme.replace('_', ' ')\n",
    "\n",
    "        # if country_nme == 'Madagascar':\n",
    "        #     print('333')\n",
    "        try:\n",
    "            country_label = df_label.loc[country_nme]\n",
    "            water_label = water_data.loc[country_nme]\n",
    "            ndvi_mean_label = ndvi_mean_df.loc[country_nme]\n",
    "            ndvi_max_label = ndvi_max_df.loc[country_nme]\n",
    "            ndvi_min_label = ndvi_min_df.loc[country_nme]\n",
    "            evi_mean_label = evi_mean_df.loc[country_nme]\n",
    "            evi_max_label = evi_max_df.loc[country_nme]\n",
    "            evi_min_label = evi_min_df.loc[country_nme]\n",
    "\n",
    "            for i in data_list:\n",
    "                year = str(i).split('_')[-1][:4]\n",
    "                data_array = np.load(str(i.absolute()))\n",
    "#                 for j in data_array:\n",
    "#                     counter += Counter(j)[0]\n",
    "#                     sum += len(j)\n",
    "#                 if scaler_data is None:\n",
    "#                     scaler_data = data_array\n",
    "#                 data = np.transpose(data_array)\n",
    "#                 if resize_dim != 'ori':\n",
    "#                     data = im.fromarray(data).resize(resize_dim)\n",
    "#                     data = np.array(data)\n",
    "#                 for j in data:\n",
    "#                     counter_new += Counter(j)[0]\n",
    "#                     sum_new += len(j)\n",
    "                data_array = cv2.resize(data_array, (224, 224))\n",
    "                hist_3 = np.zeros((224,224, 3))\n",
    "                hist_3[:,:,0] = data_array \n",
    "                hist_3[:,:,1] = data_array\n",
    "                hist_3[:,:,2] = data_array\n",
    "                if year < '2015':\n",
    "                    train_x.append(hist_3)\n",
    "                    train_y.append(country_label[year])\n",
    "                    \n",
    "                    country_train_x.append(country_nme)\n",
    "                    \n",
    "                    water_train_x.append(water_label[year])\n",
    "                    ndvi_mean_train_x.append(ndvi_mean_label[year])\n",
    "                    ndvi_max_train_x.append(ndvi_max_label[year])\n",
    "                    ndvi_min_train_x.append(ndvi_min_label[year])\n",
    "                    evi_mean_train_x.append(evi_mean_label[year])\n",
    "                    evi_max_train_x.append(evi_max_label[year])\n",
    "                    evi_min_train_x.append(evi_min_label[year])\n",
    "                    year_train_x.append(int(year))\n",
    "\n",
    "                elif year < '2019':\n",
    "                    valid_x.append(hist_3)\n",
    "                    valid_y.append(country_label[year])\n",
    "                    \n",
    "                    country_test_x.append(country_nme)\n",
    "\n",
    "                    water_test_x.append(water_label[year])\n",
    "                    ndvi_mean_test_x.append(ndvi_mean_label[year])\n",
    "                    ndvi_max_test_x.append(ndvi_max_label[year])\n",
    "                    ndvi_min_test_x.append(ndvi_min_label[year])\n",
    "                    evi_mean_test_x.append(evi_mean_label[year])\n",
    "                    evi_max_test_x.append(evi_max_label[year])\n",
    "                    evi_min_test_x.append(evi_min_label[year])\n",
    "                    year_test_x.append(int(year))\n",
    "\n",
    "                else:\n",
    "                    test_x.append(hist_3)\n",
    "        except KeyError:\n",
    "            print(country_nme, 'label dataset is missing.', 'Passed ', country_nme, 'data.')\n",
    "    print('-'*50)\n",
    "    print('Training dataset sizes:', len(train_x))\n",
    "    print('Validation dataset sizes:', len(valid_x))\n",
    "    print('Testing dataset sizes:', len(test_x))\n",
    "    # print('Number of 0:', counter)\n",
    "    # print('Number of sum element:', sum)\n",
    "    # print('Percentage of 0:', counter/sum)\n",
    "    # print('Number of 0 new:', counter_new)\n",
    "    # print('Number of sum element new:', sum_new)\n",
    "    # print('Percentage of 0 new:', counter_new/sum_new)\n",
    "\n",
    "    hybrid_model_train_data = pd.DataFrame(data={\n",
    "        'water':water_train_x,\n",
    "        'ndvi_mean':ndvi_mean_train_x,\n",
    "        # 'ndvi_max':ndvi_max_train_x,\n",
    "        # 'ndvi_min':ndvi_min_train_x,\n",
    "        'evi_mean':evi_mean_train_x,\n",
    "        # 'evi_max':evi_max_train_x,\n",
    "        # 'evi_min':evi_min_train_x,\n",
    "        #'year': year_train_x,\n",
    "        'country':country_train_x\n",
    "    })\n",
    "\n",
    "    hybrid_model_test_data = pd.DataFrame(data={\n",
    "        'water':water_test_x,\n",
    "        'ndvi_mean':ndvi_mean_test_x,\n",
    "        # 'ndvi_max':ndvi_max_test_x,\n",
    "        # 'ndvi_min':ndvi_min_test_x,\n",
    "        'evi_mean':evi_mean_test_x,\n",
    "        # 'evi_max':evi_max_test_x,\n",
    "        # 'evi_min':evi_min_test_x,\n",
    "        #'year':year_test_x,\n",
    "        'country':country_test_x\n",
    "    })\n",
    "\n",
    "    return np.array(train_x), np.array(train_y), np.array(test_x), np.array(valid_x), np.array(valid_y), hybrid_model_train_data, hybrid_model_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "564abd68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Range Before Scale: 221.0  to  0.2176\n",
      "Data Range After Scale: 2.0  to  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\talha\\anaconda3\\envs\\EffNet\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n",
      "C:\\Users\\talha\\anaconda3\\envs\\EffNet\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n",
      "C:\\Users\\talha\\anaconda3\\envs\\EffNet\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n",
      "C:\\Users\\talha\\anaconda3\\envs\\EffNet\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n",
      "C:\\Users\\talha\\anaconda3\\envs\\EffNet\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n",
      "C:\\Users\\talha\\anaconda3\\envs\\EffNet\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n",
      "C:\\Users\\talha\\anaconda3\\envs\\EffNet\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n",
      "C:\\Users\\talha\\anaconda3\\envs\\EffNet\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n",
      "C:\\Users\\talha\\anaconda3\\envs\\EffNet\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n",
      "C:\\Users\\talha\\anaconda3\\envs\\EffNet\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n",
      "C:\\Users\\talha\\anaconda3\\envs\\EffNet\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n",
      "C:\\Users\\talha\\anaconda3\\envs\\EffNet\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n",
      "C:\\Users\\talha\\anaconda3\\envs\\EffNet\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n",
      "C:\\Users\\talha\\anaconda3\\envs\\EffNet\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n",
      "C:\\Users\\talha\\anaconda3\\envs\\EffNet\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n"
     ]
    }
   ],
   "source": [
    "label_data,scaler = get_label_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ba02a31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congo label dataset is missing. Passed  Congo data.\n",
      "--------------------------------------------------\n",
      "Training dataset sizes: 299\n",
      "Validation dataset sizes: 23\n",
      "Testing dataset sizes: 0\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, x_valid, y_valid, h_df_train, h_df_test = data_extraction_and_batching(label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a71f73be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>water</th>\n",
       "      <th>ndvi_mean</th>\n",
       "      <th>evi_mean</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>178</td>\n",
       "      <td>0.115041</td>\n",
       "      <td>0.083091</td>\n",
       "      <td>Algeria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>212</td>\n",
       "      <td>0.119968</td>\n",
       "      <td>0.086220</td>\n",
       "      <td>Algeria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>223</td>\n",
       "      <td>0.122355</td>\n",
       "      <td>0.087475</td>\n",
       "      <td>Algeria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>199</td>\n",
       "      <td>0.119373</td>\n",
       "      <td>0.085392</td>\n",
       "      <td>Algeria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>213</td>\n",
       "      <td>0.121695</td>\n",
       "      <td>0.087041</td>\n",
       "      <td>Algeria</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   water  ndvi_mean  evi_mean  country\n",
       "0    178   0.115041  0.083091  Algeria\n",
       "1    212   0.119968  0.086220  Algeria\n",
       "2    223   0.122355  0.087475  Algeria\n",
       "3    199   0.119373  0.085392  Algeria\n",
       "4    213   0.121695  0.087041  Algeria"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "40a096eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>water</th>\n",
       "      <th>ndvi_mean</th>\n",
       "      <th>evi_mean</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>218</td>\n",
       "      <td>0.121690</td>\n",
       "      <td>0.087120</td>\n",
       "      <td>Algeria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>124</td>\n",
       "      <td>0.542868</td>\n",
       "      <td>0.306535</td>\n",
       "      <td>Angola</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23</td>\n",
       "      <td>0.459748</td>\n",
       "      <td>0.300933</td>\n",
       "      <td>Benin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104</td>\n",
       "      <td>0.301978</td>\n",
       "      <td>0.192956</td>\n",
       "      <td>Botswana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33</td>\n",
       "      <td>0.338379</td>\n",
       "      <td>0.224385</td>\n",
       "      <td>Burkina Faso</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>178</td>\n",
       "      <td>0.207719</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>Chad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>108</td>\n",
       "      <td>0.495889</td>\n",
       "      <td>0.323549</td>\n",
       "      <td>Gabon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>279</td>\n",
       "      <td>0.489793</td>\n",
       "      <td>0.332035</td>\n",
       "      <td>Ghana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1293</td>\n",
       "      <td>0.449015</td>\n",
       "      <td>0.265132</td>\n",
       "      <td>Malawi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>136</td>\n",
       "      <td>0.193783</td>\n",
       "      <td>0.133595</td>\n",
       "      <td>Mali</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>69</td>\n",
       "      <td>0.115689</td>\n",
       "      <td>0.087186</td>\n",
       "      <td>Mauritania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>71</td>\n",
       "      <td>0.216144</td>\n",
       "      <td>0.134155</td>\n",
       "      <td>Morocco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>66</td>\n",
       "      <td>0.217851</td>\n",
       "      <td>0.133541</td>\n",
       "      <td>Namibia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>155</td>\n",
       "      <td>0.125714</td>\n",
       "      <td>0.096460</td>\n",
       "      <td>Niger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>386</td>\n",
       "      <td>0.400738</td>\n",
       "      <td>0.276099</td>\n",
       "      <td>Nigeria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>115</td>\n",
       "      <td>0.550277</td>\n",
       "      <td>0.341541</td>\n",
       "      <td>Rwanda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>86</td>\n",
       "      <td>0.344573</td>\n",
       "      <td>0.232292</td>\n",
       "      <td>Senegal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>36</td>\n",
       "      <td>0.609505</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>Sierra Leone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>67</td>\n",
       "      <td>0.248513</td>\n",
       "      <td>0.163112</td>\n",
       "      <td>Somalia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>64</td>\n",
       "      <td>0.176225</td>\n",
       "      <td>0.126165</td>\n",
       "      <td>Tunisia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2228</td>\n",
       "      <td>0.541826</td>\n",
       "      <td>0.336286</td>\n",
       "      <td>Uganda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1008</td>\n",
       "      <td>0.538642</td>\n",
       "      <td>0.303983</td>\n",
       "      <td>Zambia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>418</td>\n",
       "      <td>0.438306</td>\n",
       "      <td>0.257211</td>\n",
       "      <td>Zimbabwe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    water  ndvi_mean  evi_mean       country\n",
       "0     218   0.121690  0.087120       Algeria\n",
       "1     124   0.542868  0.306535        Angola\n",
       "2      23   0.459748  0.300933         Benin\n",
       "3     104   0.301978  0.192956      Botswana\n",
       "4      33   0.338379  0.224385  Burkina Faso\n",
       "5     178   0.207719  0.141267          Chad\n",
       "6     108   0.495889  0.323549         Gabon\n",
       "7     279   0.489793  0.332035         Ghana\n",
       "8    1293   0.449015  0.265132        Malawi\n",
       "9     136   0.193783  0.133595          Mali\n",
       "10     69   0.115689  0.087186    Mauritania\n",
       "11     71   0.216144  0.134155       Morocco\n",
       "12     66   0.217851  0.133541       Namibia\n",
       "13    155   0.125714  0.096460         Niger\n",
       "14    386   0.400738  0.276099       Nigeria\n",
       "15    115   0.550277  0.341541        Rwanda\n",
       "16     86   0.344573  0.232292       Senegal\n",
       "17     36   0.609505  0.403034  Sierra Leone\n",
       "18     67   0.248513  0.163112       Somalia\n",
       "19     64   0.176225  0.126165       Tunisia\n",
       "20   2228   0.541826  0.336286        Uganda\n",
       "21   1008   0.538642  0.303983        Zambia\n",
       "22    418   0.438306  0.257211      Zimbabwe"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "732e43d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "zipBinarizer = LabelBinarizer().fit(h_df_train[\"country\"])\n",
    "trainCategorical = zipBinarizer.transform(h_df_train[\"country\"])\n",
    "testCategorical = zipBinarizer.transform(h_df_test[\"country\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1c99513a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 23)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testCategorical.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "305302b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# continuous = [\"bedrooms\", \"bathrooms\", \"area\"]\n",
    "# # performin min-max scaling each continuous feature column to\n",
    "# # the range [0, 1]\n",
    "# cs = MinMaxScaler()\n",
    "# trainContinuous = cs.fit_transform(train[continuous])\n",
    "# testContinuous = cs.transform(test[continuous])\n",
    "# # one-hot encode the zip code categorical data (by definition of\n",
    "# # one-hot encoding, all output features are now in the range [0, 1])\n",
    "# zipBinarizer = LabelBinarizer().fit(df[\"zipcode\"])\n",
    "# trainCategorical = zipBinarizer.transform(train[\"zipcode\"])\n",
    "# testCategorical = zipBinarizer.transform(test[\"zipcode\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a4593dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = MinMaxScaler()\n",
    "cont = ['water','ndvi_mean','evi_mean']\n",
    "scale = cs.fit(h_df_train[cont])\n",
    "trainContinuous = scale.transform(h_df_train[cont])\n",
    "testContinuous = scale.transform(h_df_test[cont])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "79df8aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "testX=np.hstack([testContinuous,testCategorical])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "41402a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX=np.hstack([trainContinuous,trainCategorical])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fb091d00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.070880</td>\n",
       "      <td>0.011981</td>\n",
       "      <td>0.003260</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.086230</td>\n",
       "      <td>0.021318</td>\n",
       "      <td>0.012720</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.091196</td>\n",
       "      <td>0.025842</td>\n",
       "      <td>0.016517</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.080361</td>\n",
       "      <td>0.020191</td>\n",
       "      <td>0.010218</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.086682</td>\n",
       "      <td>0.024590</td>\n",
       "      <td>0.015203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>0.204966</td>\n",
       "      <td>0.663273</td>\n",
       "      <td>0.562020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>0.194131</td>\n",
       "      <td>0.632242</td>\n",
       "      <td>0.522815</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>0.173815</td>\n",
       "      <td>0.590340</td>\n",
       "      <td>0.504178</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>0.188262</td>\n",
       "      <td>0.602210</td>\n",
       "      <td>0.531614</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>0.215350</td>\n",
       "      <td>0.647458</td>\n",
       "      <td>0.565804</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>299 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2    3    4    5    6    7    8    9   ...  \\\n",
       "0    0.070880  0.011981  0.003260  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "1    0.086230  0.021318  0.012720  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "2    0.091196  0.025842  0.016517  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "3    0.080361  0.020191  0.010218  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "4    0.086682  0.024590  0.015203  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "..        ...       ...       ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "294  0.204966  0.663273  0.562020  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "295  0.194131  0.632242  0.522815  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "296  0.173815  0.590340  0.504178  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "297  0.188262  0.602210  0.531614  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "298  0.215350  0.647458  0.565804  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "\n",
       "      16   17   18   19   20   21   22   23   24   25  \n",
       "0    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "..   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "294  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
       "295  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
       "296  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
       "297  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
       "298  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
       "\n",
       "[299 rows x 26 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(trainX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "34e65f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp(dim):\n",
    "    # define multi layer perceptron network\n",
    "    model = Sequential()\n",
    "    model.add(layers.Dense(8, input_dim=dim, activation=\"relu\"))\n",
    "    model.add(layers.Dense(4, activation=\"relu\"))\n",
    "    # return our model\n",
    "    return model\n",
    "\n",
    "def create_cnn(width, height, depth, filters=(16, 32, 64)):\n",
    "    # initialize the input shape and channel dimension, assuming\n",
    "    # TensorFlow/channels-last ordering\n",
    "    inputShape = (height, width, depth)\n",
    "    chanDim = -1\n",
    "    # define the model input\n",
    "    inputs = layers.Input(shape=inputShape)\n",
    "    # loop over the number of filters\n",
    "    for (i, f) in enumerate(filters):\n",
    "        # if this is the first CONV layer then set the input\n",
    "        # appropriately\n",
    "        if i == 0:\n",
    "            x = inputs\n",
    "        # CONV => RELU => BN => POOL\n",
    "        x = layers.Conv2D(f, (3, 3), padding=\"same\")(x)\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.BatchNormalization(axis=chanDim)(x)\n",
    "        x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    # flatten the volume, then FC => RELU => BN => DROPOUT\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(16)(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.BatchNormalization(axis=chanDim)(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    # apply another FC layer, this one to match the number of nodes\n",
    "    # coming out of the MLP\n",
    "    x = layers.Dense(4)(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    \n",
    "    # construct the CNN\n",
    "    model = Model(inputs, x)\n",
    "    # return the CNN\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c5e8d9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\talha\\anaconda3\\envs\\EffNet\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training model...\n",
      "Epoch 1/200\n",
      "38/38 [==============================] - 8s 186ms/step - loss: 86832.1641 - val_loss: 147.5152\n",
      "Epoch 2/200\n",
      "38/38 [==============================] - 6s 163ms/step - loss: 524197.8750 - val_loss: 328.7755\n",
      "Epoch 3/200\n",
      "38/38 [==============================] - 7s 171ms/step - loss: 819916.0625 - val_loss: 134.2984\n",
      "Epoch 4/200\n",
      "38/38 [==============================] - 6s 162ms/step - loss: 205653.5781 - val_loss: 240.0915\n",
      "Epoch 5/200\n",
      "38/38 [==============================] - 6s 157ms/step - loss: 90994.7578 - val_loss: 329.4099\n",
      "Epoch 6/200\n",
      "38/38 [==============================] - 6s 150ms/step - loss: 133474.7500 - val_loss: 264.3687\n",
      "Epoch 7/200\n",
      "38/38 [==============================] - 6s 153ms/step - loss: 1443147.3750 - val_loss: 345.0956\n",
      "Epoch 8/200\n",
      "38/38 [==============================] - 6s 154ms/step - loss: 174214.5938 - val_loss: 259.2484\n",
      "Epoch 9/200\n",
      "38/38 [==============================] - 6s 150ms/step - loss: 362116.0000 - val_loss: 342.6041\n",
      "Epoch 10/200\n",
      "38/38 [==============================] - 6s 148ms/step - loss: 61984.1484 - val_loss: 322.1900\n",
      "Epoch 11/200\n",
      "38/38 [==============================] - 6s 156ms/step - loss: 40541.1211 - val_loss: 259.6555\n",
      "Epoch 12/200\n",
      "38/38 [==============================] - 6s 152ms/step - loss: 36814.5039 - val_loss: 268.3804\n",
      "Epoch 13/200\n",
      "38/38 [==============================] - 6s 148ms/step - loss: 87957.8984 - val_loss: 322.3568\n",
      "Epoch 14/200\n",
      "38/38 [==============================] - 6s 154ms/step - loss: 26737.5977 - val_loss: 347.3406\n",
      "Epoch 15/200\n",
      "38/38 [==============================] - 6s 156ms/step - loss: 10643.1504 - val_loss: 397.4704\n",
      "Epoch 16/200\n",
      "38/38 [==============================] - 6s 151ms/step - loss: 932548.2500 - val_loss: 76.6383\n",
      "Epoch 17/200\n",
      "38/38 [==============================] - 6s 153ms/step - loss: 10011.5215 - val_loss: 133.2863\n",
      "Epoch 18/200\n",
      "38/38 [==============================] - 6s 157ms/step - loss: 27073.4238 - val_loss: 83.3764\n",
      "Epoch 19/200\n",
      "38/38 [==============================] - 6s 154ms/step - loss: 9890.4297 - val_loss: 157.0093\n",
      "Epoch 20/200\n",
      "38/38 [==============================] - 6s 151ms/step - loss: 14228.5088 - val_loss: 84.8317\n",
      "Epoch 21/200\n",
      "38/38 [==============================] - 6s 157ms/step - loss: 10237.1055 - val_loss: 157.3412\n",
      "Epoch 22/200\n",
      "38/38 [==============================] - 6s 162ms/step - loss: 10367.5176 - val_loss: 88.7151\n",
      "Epoch 23/200\n",
      "38/38 [==============================] - 6s 165ms/step - loss: 9724.4199 - val_loss: 171.1225\n",
      "Epoch 24/200\n",
      "38/38 [==============================] - 6s 164ms/step - loss: 10138.3906 - val_loss: 153.7615\n",
      "Epoch 25/200\n",
      "38/38 [==============================] - 6s 166ms/step - loss: 4012.6860 - val_loss: 178.9912\n",
      "Epoch 26/200\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 9577.7461 - val_loss: 103.6871\n",
      "Epoch 27/200\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 11080.2070 - val_loss: 177.2561\n",
      "Epoch 28/200\n",
      "38/38 [==============================] - 6s 166ms/step - loss: 9275.9199 - val_loss: 107.3526\n",
      "Epoch 29/200\n",
      "38/38 [==============================] - 6s 157ms/step - loss: 11504.0410 - val_loss: 145.9871\n",
      "Epoch 30/200\n",
      "38/38 [==============================] - 6s 157ms/step - loss: 9186.9697 - val_loss: 139.7279\n",
      "Epoch 31/200\n",
      "38/38 [==============================] - 6s 159ms/step - loss: 11497.3223 - val_loss: 105.9658\n",
      "Epoch 32/200\n",
      "38/38 [==============================] - 6s 160ms/step - loss: 4103.7651 - val_loss: 93.1524\n",
      "Epoch 33/200\n",
      "38/38 [==============================] - 6s 160ms/step - loss: 11545.5410 - val_loss: 149.5700\n",
      "Epoch 34/200\n",
      "38/38 [==============================] - 6s 159ms/step - loss: 7982.7573 - val_loss: 88.4799\n",
      "Epoch 35/200\n",
      "38/38 [==============================] - 6s 160ms/step - loss: 11589.9473 - val_loss: 92.2893\n",
      "Epoch 36/200\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 5092.8677 - val_loss: 82.0266\n",
      "Epoch 37/200\n",
      "38/38 [==============================] - 6s 164ms/step - loss: 11386.7998 - val_loss: 114.6567\n",
      "Epoch 38/200\n",
      "38/38 [==============================] - 6s 162ms/step - loss: 4675.9365 - val_loss: 79.3830\n",
      "Epoch 39/200\n",
      "38/38 [==============================] - 6s 157ms/step - loss: 11092.9619 - val_loss: 152.3913\n",
      "Epoch 40/200\n",
      "38/38 [==============================] - 6s 159ms/step - loss: 9109.6533 - val_loss: 101.5377\n",
      "Epoch 41/200\n",
      "38/38 [==============================] - 6s 167ms/step - loss: 11284.0811 - val_loss: 126.1247\n",
      "Epoch 42/200\n",
      "38/38 [==============================] - 6s 160ms/step - loss: 7656.1128 - val_loss: 78.7438\n",
      "Epoch 43/200\n",
      "38/38 [==============================] - 6s 157ms/step - loss: 11550.3525 - val_loss: 130.3187\n",
      "Epoch 44/200\n",
      "38/38 [==============================] - 6s 157ms/step - loss: 8753.7803 - val_loss: 120.0100\n",
      "Epoch 45/200\n",
      "38/38 [==============================] - 6s 164ms/step - loss: 10338.9688 - val_loss: 138.7298\n",
      "Epoch 46/200\n",
      "38/38 [==============================] - 6s 165ms/step - loss: 8314.5654 - val_loss: 78.6729\n",
      "Epoch 47/200\n",
      "38/38 [==============================] - 6s 160ms/step - loss: 11260.7207 - val_loss: 142.0136\n",
      "Epoch 48/200\n",
      "38/38 [==============================] - 6s 156ms/step - loss: 7977.6724 - val_loss: 73.3535\n",
      "Epoch 49/200\n",
      "38/38 [==============================] - 6s 170ms/step - loss: 12218.8438 - val_loss: 86.1802\n",
      "Epoch 50/200\n",
      "38/38 [==============================] - 7s 181ms/step - loss: 4978.7031 - val_loss: 72.2726\n",
      "Epoch 51/200\n",
      "38/38 [==============================] - 6s 161ms/step - loss: 11739.0469 - val_loss: 141.1326\n",
      "Epoch 52/200\n",
      "38/38 [==============================] - 6s 160ms/step - loss: 8164.3730 - val_loss: 77.9872\n",
      "Epoch 53/200\n",
      "38/38 [==============================] - 6s 150ms/step - loss: 12302.2695 - val_loss: 79.9878\n",
      "Epoch 54/200\n",
      "38/38 [==============================] - 6s 152ms/step - loss: 7782.1699 - val_loss: 104.0449\n",
      "Epoch 55/200\n",
      "38/38 [==============================] - 6s 157ms/step - loss: 674.6032 - val_loss: 142.4486\n",
      "Epoch 56/200\n",
      "38/38 [==============================] - 6s 167ms/step - loss: 7866.3125 - val_loss: 69.6236\n",
      "Epoch 57/200\n",
      "38/38 [==============================] - 6s 166ms/step - loss: 11773.8535 - val_loss: 138.5846\n",
      "Epoch 58/200\n",
      "38/38 [==============================] - 6s 167ms/step - loss: 7622.2485 - val_loss: 68.0423\n",
      "Epoch 59/200\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 12701.4902 - val_loss: 75.6790\n",
      "Epoch 60/200\n",
      "38/38 [==============================] - 6s 170ms/step - loss: 1258.9976 - val_loss: 229.5933\n",
      "Epoch 61/200\n",
      "38/38 [==============================] - 7s 179ms/step - loss: 27049.8516 - val_loss: 139.6818\n",
      "Epoch 62/200\n",
      "38/38 [==============================] - 7s 191ms/step - loss: 7449.0977 - val_loss: 91.8213\n",
      "Epoch 63/200\n",
      "38/38 [==============================] - 7s 185ms/step - loss: 11964.5352 - val_loss: 121.8092\n",
      "Epoch 64/200\n",
      "38/38 [==============================] - 7s 183ms/step - loss: 6833.3774 - val_loss: 65.8480\n",
      "Epoch 65/200\n",
      "38/38 [==============================] - 6s 166ms/step - loss: 13284.5449 - val_loss: 93.1123\n",
      "Epoch 66/200\n",
      "38/38 [==============================] - 6s 162ms/step - loss: 6320.9028 - val_loss: 67.1979\n",
      "Epoch 67/200\n",
      "38/38 [==============================] - 6s 163ms/step - loss: 13027.7773 - val_loss: 116.8945\n",
      "Epoch 68/200\n",
      "38/38 [==============================] - 6s 163ms/step - loss: 6484.4351 - val_loss: 63.9514\n",
      "Epoch 69/200\n",
      "38/38 [==============================] - 7s 172ms/step - loss: 13283.7080 - val_loss: 112.7365\n",
      "Epoch 70/200\n",
      "38/38 [==============================] - 7s 185ms/step - loss: 4715.8911 - val_loss: 63.7534\n",
      "Epoch 71/200\n",
      "38/38 [==============================] - 7s 182ms/step - loss: 13225.8594 - val_loss: 126.9870\n",
      "Epoch 72/200\n",
      "38/38 [==============================] - 7s 185ms/step - loss: 6753.0801 - val_loss: 91.9174\n",
      "Epoch 73/200\n",
      "38/38 [==============================] - 6s 167ms/step - loss: 11123.3262 - val_loss: 123.6040\n",
      "Epoch 74/200\n",
      "38/38 [==============================] - 8s 221ms/step - loss: 6198.3975 - val_loss: 63.8483\n",
      "Epoch 75/200\n",
      "38/38 [==============================] - 7s 180ms/step - loss: 13639.2168 - val_loss: 119.5899\n",
      "Epoch 76/200\n",
      "38/38 [==============================] - 6s 160ms/step - loss: 6282.6216 - val_loss: 74.3671\n",
      "Epoch 77/200\n",
      "38/38 [==============================] - 7s 174ms/step - loss: 10050.5215 - val_loss: 125.3856\n",
      "Epoch 78/200\n",
      "38/38 [==============================] - 7s 180ms/step - loss: 5498.1587 - val_loss: 63.6339\n",
      "Epoch 79/200\n",
      "38/38 [==============================] - 6s 166ms/step - loss: 14207.0000 - val_loss: 111.0431\n",
      "Epoch 80/200\n",
      "38/38 [==============================] - 6s 160ms/step - loss: 4382.7163 - val_loss: 63.6783\n",
      "Epoch 81/200\n",
      "38/38 [==============================] - 6s 160ms/step - loss: 13887.4004 - val_loss: 124.0555\n",
      "Epoch 82/200\n",
      "38/38 [==============================] - 6s 160ms/step - loss: 5615.1177 - val_loss: 63.6707\n",
      "Epoch 83/200\n",
      "38/38 [==============================] - 6s 163ms/step - loss: 14103.0986 - val_loss: 104.9193\n",
      "Epoch 84/200\n",
      "38/38 [==============================] - 6s 163ms/step - loss: 1630.3491 - val_loss: 63.6525\n",
      "Epoch 85/200\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 13765.4609 - val_loss: 126.4051\n",
      "Epoch 86/200\n",
      "38/38 [==============================] - 6s 160ms/step - loss: 5722.0435 - val_loss: 63.6474\n",
      "Epoch 87/200\n",
      "38/38 [==============================] - 6s 160ms/step - loss: 14067.2559 - val_loss: 119.5854\n",
      "Epoch 88/200\n",
      "38/38 [==============================] - 6s 156ms/step - loss: 6017.7021 - val_loss: 65.2026\n",
      "Epoch 89/200\n",
      "38/38 [==============================] - 6s 155ms/step - loss: 14231.5518 - val_loss: 64.0544\n",
      "Epoch 90/200\n",
      "38/38 [==============================] - 6s 153ms/step - loss: 466.8008 - val_loss: 63.6831\n",
      "Epoch 91/200\n",
      "38/38 [==============================] - 6s 153ms/step - loss: 14096.3682 - val_loss: 83.6474\n",
      "Epoch 92/200\n",
      "38/38 [==============================] - 6s 152ms/step - loss: 4319.0308 - val_loss: 63.7927\n",
      "Epoch 93/200\n",
      "38/38 [==============================] - 6s 157ms/step - loss: 13759.9678 - val_loss: 120.4810\n",
      "Epoch 94/200\n",
      "38/38 [==============================] - 6s 153ms/step - loss: 6072.5200 - val_loss: 95.1292\n",
      "Epoch 95/200\n",
      "38/38 [==============================] - 6s 153ms/step - loss: 13948.1943 - val_loss: 87.4359\n",
      "Epoch 96/200\n",
      "38/38 [==============================] - 6s 153ms/step - loss: 5378.8862 - val_loss: 67.2221\n",
      "Epoch 97/200\n",
      "38/38 [==============================] - 6s 157ms/step - loss: 14345.8174 - val_loss: 78.0811\n",
      "Epoch 98/200\n",
      "38/38 [==============================] - 6s 150ms/step - loss: 5498.2168 - val_loss: 81.3293\n",
      "Epoch 99/200\n",
      "38/38 [==============================] - 6s 152ms/step - loss: 13378.7881 - val_loss: 112.7168\n",
      "Epoch 100/200\n",
      "38/38 [==============================] - 6s 157ms/step - loss: 5208.9917 - val_loss: 63.8661\n",
      "Epoch 101/200\n",
      "38/38 [==============================] - 6s 158ms/step - loss: 14613.4736 - val_loss: 99.0050\n",
      "Epoch 102/200\n",
      "38/38 [==============================] - 6s 153ms/step - loss: 5348.2036 - val_loss: 106.5942\n",
      "Epoch 103/200\n",
      "38/38 [==============================] - 6s 158ms/step - loss: 1830.4519 - val_loss: 123.3345\n",
      "Epoch 104/200\n",
      "38/38 [==============================] - 6s 158ms/step - loss: 5102.2178 - val_loss: 63.6537\n",
      "Epoch 105/200\n",
      "38/38 [==============================] - 6s 152ms/step - loss: 15077.7295 - val_loss: 63.9344\n",
      "Epoch 106/200\n",
      "38/38 [==============================] - 6s 157ms/step - loss: 684.1287 - val_loss: 63.6243\n",
      "Epoch 107/200\n",
      "38/38 [==============================] - 6s 157ms/step - loss: 14807.9717 - val_loss: 110.6576\n",
      "Epoch 108/200\n",
      "38/38 [==============================] - 6s 156ms/step - loss: 5092.6675 - val_loss: 64.0238\n",
      "Epoch 109/200\n",
      "38/38 [==============================] - 6s 155ms/step - loss: 14273.7607 - val_loss: 114.1324\n",
      "Epoch 110/200\n",
      "38/38 [==============================] - 6s 158ms/step - loss: 5075.8022 - val_loss: 105.2713\n",
      "Epoch 111/200\n",
      "38/38 [==============================] - 6s 163ms/step - loss: 12315.4648 - val_loss: 114.6890\n",
      "Epoch 112/200\n",
      "38/38 [==============================] - 6s 161ms/step - loss: 4385.1333 - val_loss: 63.6054\n",
      "Epoch 113/200\n",
      "38/38 [==============================] - 6s 163ms/step - loss: 14624.9629 - val_loss: 119.0548\n",
      "Epoch 114/200\n",
      "38/38 [==============================] - 6s 159ms/step - loss: 4609.4521 - val_loss: 63.7416\n",
      "Epoch 115/200\n",
      "38/38 [==============================] - 6s 151ms/step - loss: 15434.2539 - val_loss: 92.2059\n",
      "Epoch 116/200\n",
      "38/38 [==============================] - 6s 155ms/step - loss: 2955.9736 - val_loss: 63.5796\n",
      "Epoch 117/200\n",
      "38/38 [==============================] - 6s 159ms/step - loss: 14450.2959 - val_loss: 119.0682\n",
      "Epoch 118/200\n",
      "38/38 [==============================] - 6s 163ms/step - loss: 4301.4849 - val_loss: 63.5112\n",
      "Epoch 119/200\n",
      "38/38 [==============================] - 6s 156ms/step - loss: 14931.4414 - val_loss: 118.3481\n",
      "Epoch 120/200\n",
      "38/38 [==============================] - 6s 156ms/step - loss: 4000.4988 - val_loss: 63.4558\n",
      "Epoch 121/200\n",
      "38/38 [==============================] - 6s 159ms/step - loss: 15738.2188 - val_loss: 77.3408\n",
      "Epoch 122/200\n",
      "38/38 [==============================] - 6s 170ms/step - loss: 2042.9082 - val_loss: 211.1353\n",
      "Epoch 123/200\n",
      "38/38 [==============================] - 6s 163ms/step - loss: 24333.0996 - val_loss: 142.9841\n",
      "Epoch 124/200\n",
      "38/38 [==============================] - 6s 166ms/step - loss: 6759.3184 - val_loss: 63.4639\n",
      "Epoch 125/200\n",
      "38/38 [==============================] - 6s 163ms/step - loss: 16031.9883 - val_loss: 92.6144\n",
      "Epoch 126/200\n",
      "38/38 [==============================] - 6s 156ms/step - loss: 3629.5486 - val_loss: 63.7917\n",
      "Epoch 127/200\n",
      "38/38 [==============================] - 6s 164ms/step - loss: 15952.6250 - val_loss: 73.6746\n",
      "Epoch 128/200\n",
      "38/38 [==============================] - 6s 164ms/step - loss: 3890.0674 - val_loss: 79.5346\n",
      "Epoch 129/200\n",
      "38/38 [==============================] - 6s 153ms/step - loss: 9942.2686 - val_loss: 115.8555\n",
      "Epoch 130/200\n",
      "38/38 [==============================] - 6s 152ms/step - loss: 3801.1606 - val_loss: 75.7025\n",
      "Epoch 131/200\n",
      "38/38 [==============================] - 6s 157ms/step - loss: 16162.0166 - val_loss: 75.5472\n",
      "Epoch 132/200\n",
      "38/38 [==============================] - 6s 160ms/step - loss: 429.3895 - val_loss: 63.4386\n",
      "Epoch 133/200\n",
      "38/38 [==============================] - 7s 179ms/step - loss: 16351.3213 - val_loss: 78.1499\n",
      "Epoch 134/200\n",
      "38/38 [==============================] - 6s 165ms/step - loss: 4179.7920 - val_loss: 208.0017\n",
      "Epoch 135/200\n",
      "38/38 [==============================] - 6s 164ms/step - loss: 23451.2422 - val_loss: 123.9271\n",
      "Epoch 136/200\n",
      "38/38 [==============================] - 6s 161ms/step - loss: 3324.2131 - val_loss: 77.5764\n",
      "Epoch 137/200\n",
      "38/38 [==============================] - 6s 163ms/step - loss: 16702.5020 - val_loss: 71.2387\n",
      "Epoch 138/200\n",
      "38/38 [==============================] - 6s 158ms/step - loss: 2114.9084 - val_loss: 63.6679\n",
      "Epoch 139/200\n",
      "38/38 [==============================] - 6s 156ms/step - loss: 16851.8320 - val_loss: 87.8187\n",
      "Epoch 140/200\n",
      "38/38 [==============================] - 6s 157ms/step - loss: 138.6933 - val_loss: 63.3462\n",
      "Epoch 141/200\n",
      "38/38 [==============================] - 6s 155ms/step - loss: 16588.9941 - val_loss: 111.0994\n",
      "Epoch 142/200\n",
      "38/38 [==============================] - 6s 154ms/step - loss: 2924.2869 - val_loss: 67.9182\n",
      "Epoch 143/200\n",
      "38/38 [==============================] - 6s 154ms/step - loss: 8721.2168 - val_loss: 111.1524\n",
      "Epoch 144/200\n",
      "38/38 [==============================] - 6s 155ms/step - loss: 2560.6606 - val_loss: 69.5297\n",
      "Epoch 145/200\n",
      "38/38 [==============================] - 6s 156ms/step - loss: 17252.4922 - val_loss: 79.2372\n",
      "Epoch 146/200\n",
      "38/38 [==============================] - 6s 164ms/step - loss: 1995.8618 - val_loss: 63.8167\n",
      "Epoch 147/200\n",
      "38/38 [==============================] - 7s 187ms/step - loss: 15423.8379 - val_loss: 106.2934\n",
      "Epoch 148/200\n",
      "38/38 [==============================] - 6s 170ms/step - loss: 1760.4032 - val_loss: 63.2451\n",
      "Epoch 149/200\n",
      "38/38 [==============================] - 7s 171ms/step - loss: 17804.6641 - val_loss: 91.0350\n",
      "Epoch 150/200\n",
      "38/38 [==============================] - 6s 160ms/step - loss: 1752.2853 - val_loss: 63.5030\n",
      "Epoch 151/200\n",
      "38/38 [==============================] - 6s 152ms/step - loss: 16661.3984 - val_loss: 105.7381\n",
      "Epoch 152/200\n",
      "38/38 [==============================] - 6s 159ms/step - loss: 1931.7048 - val_loss: 63.4263\n",
      "Epoch 153/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 6s 170ms/step - loss: 15896.4580 - val_loss: 106.7358\n",
      "Epoch 154/200\n",
      "38/38 [==============================] - 6s 159ms/step - loss: 1778.7321 - val_loss: 63.3993\n",
      "Epoch 155/200\n",
      "38/38 [==============================] - 6s 147ms/step - loss: 18203.2031 - val_loss: 64.5517\n",
      "Epoch 156/200\n",
      "38/38 [==============================] - 5s 144ms/step - loss: 1657.1577 - val_loss: 82.0210\n",
      "Epoch 157/200\n",
      "38/38 [==============================] - 6s 149ms/step - loss: 17927.6445 - val_loss: 63.9410\n",
      "Epoch 158/200\n",
      "38/38 [==============================] - 6s 155ms/step - loss: 739.0296 - val_loss: 63.5715\n",
      "Epoch 159/200\n",
      "38/38 [==============================] - 6s 160ms/step - loss: 17562.7266 - val_loss: 98.9489\n",
      "Epoch 160/200\n",
      "38/38 [==============================] - 6s 162ms/step - loss: 1610.6221 - val_loss: 64.7361\n",
      "Epoch 161/200\n",
      "38/38 [==============================] - 6s 156ms/step - loss: 17853.8984 - val_loss: 86.2186\n",
      "Epoch 162/200\n",
      "38/38 [==============================] - 6s 150ms/step - loss: 1401.5250 - val_loss: 76.2827\n",
      "Epoch 163/200\n",
      "38/38 [==============================] - 6s 152ms/step - loss: 17132.3359 - val_loss: 92.9816\n",
      "Epoch 164/200\n",
      "38/38 [==============================] - 6s 147ms/step - loss: 1214.1704 - val_loss: 69.0462\n",
      "Epoch 165/200\n",
      "38/38 [==============================] - 6s 152ms/step - loss: 12622.0029 - val_loss: 102.5858\n",
      "Epoch 166/200\n",
      "38/38 [==============================] - 6s 150ms/step - loss: 925.5889 - val_loss: 63.4957\n",
      "Epoch 167/200\n",
      "38/38 [==============================] - 6s 148ms/step - loss: 18113.7695 - val_loss: 97.1408\n",
      "Epoch 168/200\n",
      "38/38 [==============================] - 6s 151ms/step - loss: 576.9275 - val_loss: 63.2614\n",
      "Epoch 169/200\n",
      "38/38 [==============================] - 6s 152ms/step - loss: 17685.9004 - val_loss: 100.5466\n",
      "Epoch 170/200\n",
      "38/38 [==============================] - 6s 152ms/step - loss: 253.1864 - val_loss: 63.5674\n",
      "Epoch 171/200\n",
      "38/38 [==============================] - 6s 151ms/step - loss: 19154.2891 - val_loss: 82.3548\n",
      "Epoch 172/200\n",
      "38/38 [==============================] - 6s 150ms/step - loss: 688.8344 - val_loss: 67.7320\n",
      "Epoch 173/200\n",
      "38/38 [==============================] - 6s 156ms/step - loss: 18793.1758 - val_loss: 74.7539\n",
      "Epoch 174/200\n",
      "38/38 [==============================] - 6s 158ms/step - loss: 2399.3669 - val_loss: 191.4205\n",
      "Epoch 175/200\n",
      "38/38 [==============================] - 6s 159ms/step - loss: 19646.5371 - val_loss: 104.2408\n",
      "Epoch 176/200\n",
      "38/38 [==============================] - 6s 163ms/step - loss: 588.1796 - val_loss: 63.1848\n",
      "Epoch 177/200\n",
      "38/38 [==============================] - 6s 149ms/step - loss: 19276.0215 - val_loss: 88.1929\n",
      "Epoch 178/200\n",
      "38/38 [==============================] - 6s 149ms/step - loss: 454.2005 - val_loss: 83.7929\n",
      "Epoch 179/200\n",
      "38/38 [==============================] - 6s 159ms/step - loss: 19064.2305 - val_loss: 67.1431\n",
      "Epoch 180/200\n",
      "38/38 [==============================] - 7s 173ms/step - loss: 149.1335 - val_loss: 154.2515\n",
      "Epoch 181/200\n",
      "38/38 [==============================] - 7s 172ms/step - loss: 18905.0156 - val_loss: 110.9353\n",
      "Epoch 182/200\n",
      "38/38 [==============================] - 6s 158ms/step - loss: 506.2692 - val_loss: 63.3852\n",
      "Epoch 183/200\n",
      "38/38 [==============================] - 6s 161ms/step - loss: 19525.6230 - val_loss: 73.3701\n",
      "Epoch 184/200\n",
      "38/38 [==============================] - 6s 163ms/step - loss: 780.5820 - val_loss: 181.4261\n",
      "Epoch 185/200\n",
      "38/38 [==============================] - 6s 155ms/step - loss: 19092.4766 - val_loss: 105.0202\n",
      "Epoch 186/200\n",
      "38/38 [==============================] - 6s 153ms/step - loss: 164.2034 - val_loss: 69.3297\n",
      "Epoch 187/200\n",
      "38/38 [==============================] - 6s 154ms/step - loss: 19777.7949 - val_loss: 63.5006\n",
      "Epoch 188/200\n",
      "38/38 [==============================] - 6s 161ms/step - loss: 20251.1719 - val_loss: 152.1638\n",
      "Epoch 189/200\n",
      "38/38 [==============================] - 6s 153ms/step - loss: 10151.1436 - val_loss: 94.7897\n",
      "Epoch 190/200\n",
      "38/38 [==============================] - 6s 150ms/step - loss: 7420.2275 - val_loss: 134.1324\n",
      "Epoch 191/200\n",
      "38/38 [==============================] - 6s 153ms/step - loss: 8189.4287 - val_loss: 70.2093\n",
      "Epoch 192/200\n",
      "38/38 [==============================] - 6s 154ms/step - loss: 8223.7021 - val_loss: 140.5914\n",
      "Epoch 193/200\n",
      "38/38 [==============================] - 6s 161ms/step - loss: 9766.1074 - val_loss: 128.9809\n",
      "Epoch 194/200\n",
      "38/38 [==============================] - 6s 155ms/step - loss: 7224.9390 - val_loss: 131.4889\n",
      "Epoch 195/200\n",
      "38/38 [==============================] - 6s 157ms/step - loss: 8435.3027 - val_loss: 69.5647\n",
      "Epoch 196/200\n",
      "38/38 [==============================] - 6s 157ms/step - loss: 8695.0176 - val_loss: 139.1408\n",
      "Epoch 197/200\n",
      "38/38 [==============================] - 6s 160ms/step - loss: 8954.0117 - val_loss: 68.8120\n",
      "Epoch 198/200\n",
      "38/38 [==============================] - 6s 166ms/step - loss: 9107.5459 - val_loss: 87.3052\n",
      "Epoch 199/200\n",
      "38/38 [==============================] - 6s 163ms/step - loss: 8837.9482 - val_loss: 83.3201\n",
      "Epoch 200/200\n",
      "38/38 [==============================] - 6s 152ms/step - loss: 5195.1582 - val_loss: 141.4836\n",
      "[INFO] predicting house prices...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'testY' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [135]\u001b[0m, in \u001b[0;36m<cell line: 30>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m preds \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict([testX, x_valid])\n\u001b[0;32m     29\u001b[0m diff \u001b[38;5;241m=\u001b[39m preds\u001b[38;5;241m.\u001b[39mflatten() \u001b[38;5;241m-\u001b[39m y_valid\n\u001b[1;32m---> 30\u001b[0m percentDiff \u001b[38;5;241m=\u001b[39m (diff \u001b[38;5;241m/\u001b[39m \u001b[43mtestY\u001b[49m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m     31\u001b[0m absPercentDiff \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(percentDiff)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# compute the mean and standard deviation of the absolute percentage\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# difference\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'testY' is not defined"
     ]
    }
   ],
   "source": [
    "# create the MLP and CNN models\n",
    "mlp = create_mlp(trainX.shape[1])\n",
    "cnn = create_cnn(224, 224, 3)\n",
    "# create the input to our final set of layers as the *output* of both\n",
    "# the MLP and CNN\n",
    "combinedInput = layers.concatenate([mlp.output, cnn.output])\n",
    "# our final FC layer head will have two dense layers, the final one\n",
    "# being our regression head\n",
    "x = layers.Dense(4, activation=\"relu\")(combinedInput)\n",
    "x = layers.Dense(1, activation=\"linear\")(x)\n",
    "# our final model will accept categorical/numerical data on the MLP\n",
    "# images on the CNN input, outputting a single value (crop yield)\n",
    "model = Model(inputs=[mlp.input, cnn.input], outputs=x)\n",
    "# compile the model using mean absolute percentage error as our loss\n",
    "opt = Adam(lr=1e-3, decay=1e-3 / 200)\n",
    "model.compile(loss=\"mean_absolute_percentage_error\", optimizer=opt)\n",
    "# train the model\n",
    "print(\"Training model...\")\n",
    "model.fit(\n",
    "    x=[trainX, x_train], y=y_train,\n",
    "    validation_data=([testX, x_valid], y_valid),\n",
    "    epochs=200, batch_size=8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "37a4f57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics on validation Data...\n",
      "[INFO] mean: 100.00%, std: 0.00%\n"
     ]
    }
   ],
   "source": [
    "print(\"Metrics on validation Data...\")\n",
    "preds = model.predict([testX, x_valid])\n",
    "y_valid = scaler.inverse_transform(y_valid.reshape(-1, 1))\n",
    "preds = scaler.inverse_transform(preds.reshape(-1, 1))\n",
    "diff = preds.flatten() - y_valid\n",
    "percentDiff = (diff / y_valid) * 100\n",
    "absPercentDiff = np.abs(percentDiff)\n",
    "# compute the mean and standard deviation of the absolute percentage\n",
    "# difference\n",
    "mean = np.mean(absPercentDiff)\n",
    "std = np.std(absPercentDiff)\n",
    "print(\"[INFO] mean: {:.2f}%, std: {:.2f}%\".format(mean, std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c755e084",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "valid_rmse = mean_squared_error(y_valid, preds, squared=False)\n",
    "valid_mape = abs((y_valid - preds) / y_valid).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "cb54fb36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0692709203516448"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "8efa3c10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5069006879516726"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0fe724",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
