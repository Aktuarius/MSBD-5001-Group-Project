{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa8ac1f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mdn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 17>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image \u001b[38;5;28;01mas\u001b[39;00m im\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmdn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MDN, get_mixture_loss_func,sample_from_output\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MinMaxScaler\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m History, Callback\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'mdn'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from autogluon.tabular import TabularPredictor\n",
    "from pathlib import Path\n",
    "from tensorflow.keras.models import Sequential, clone_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, InputLayer, Flatten\n",
    "from tensorflow.keras.optimizers import Adamax, Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError, MeanAbsoluteError\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "from collections import Counter\n",
    "from PIL import Image as im\n",
    "from mdn import MDN, get_mixture_loss_func,sample_from_output\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.callbacks import History, Callback\n",
    "from tensorflow.keras import metrics\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "\n",
    "def get_selected_africa_coutries_list() -> list:\n",
    "    processed_path = Path(str(Path(os.getcwd()).parent.parent.absolute()) + '/ProcessedHistograms')\n",
    "    country_list = sorted(processed_path.glob('*'))\n",
    "    return [str(country).split('\\\\')[-1].replace('_', ' ') for country in country_list]\n",
    "\n",
    "\n",
    "def get_water_data() -> pd.DataFrame:\n",
    "    countries_list_nme = get_selected_africa_coutries_list()\n",
    "    processed_path = Path(str(Path(os.getcwd()).parent.parent.absolute()) + '/WaterProcessed')\n",
    "    country_list = sorted(processed_path.glob('*'))\n",
    "    data = []\n",
    "    for i in country_list:\n",
    "        f = open(str(i))\n",
    "        water_data = json.load(f)\n",
    "        country_name =  str(i).split('\\\\')[-1].split('.')[-2][:-5].replace('_', ' ')\n",
    "        water_data['country_nme'] = country_name\n",
    "        if country_name in countries_list_nme:\n",
    "            data.append(water_data)\n",
    "    water_df = pd.DataFrame(data=data)\n",
    "    water_df = water_df.set_index('country_nme')\n",
    "    return water_df\n",
    "\n",
    "def get_ndvi_evi_data() -> [pd.DataFrame, pd.DataFrame]:\n",
    "    countries_list_nme = get_selected_africa_coutries_list()\n",
    "    processed_path = Path(str(Path(os.getcwd()).parent.parent.absolute()) + '/NDVI/Processed_edvi_data')\n",
    "    country_list = sorted(processed_path.glob('*'))\n",
    "    ndvi = []\n",
    "    evi = []\n",
    "    ndvi_index_list = None\n",
    "    evi_index_list = None\n",
    "    for country in country_list:\n",
    "        data_list = sorted(country.glob('*.csv'))\n",
    "        country_nme = str(country).split('\\\\')[-1]\n",
    "        country_nme = country_nme.replace('_', ' ')\n",
    "        if country_nme not in countries_list_nme:\n",
    "            continue\n",
    "        for i in data_list:\n",
    "            file_name = str(i).split('\\\\')[-1].split('_')[0]\n",
    "            temp_data = pd.read_csv(str(i), index_col = 0)\n",
    "            temp_data.columns = [country_nme]\n",
    "\n",
    "            if file_name == 'NDVI':\n",
    "                if ndvi_index_list is None:\n",
    "                    ndvi_index_list = temp_data.index.tolist()\n",
    "                temp_data = temp_data.reset_index(drop=True)\n",
    "                ndvi.append(temp_data)\n",
    "            else:\n",
    "                if evi_index_list is None:\n",
    "                    evi_index_list = temp_data.index.tolist()\n",
    "                temp_data = temp_data.reset_index(drop=True)\n",
    "                evi.append(temp_data)\n",
    "    ndvi_df = pd.concat(ndvi, axis=1)\n",
    "    evi_df = pd.concat(evi, axis=1)\n",
    "    ndvi_df.index = ndvi_index_list\n",
    "    evi_df.index = evi_index_list\n",
    "    return ndvi_df, evi_df\n",
    "\n",
    "\n",
    "def ndvi_evi_monthly_to_yearly(ndvi, evi):\n",
    "    ndvi = ndvi.reset_index()\n",
    "    evi = evi.reset_index()\n",
    "\n",
    "    ndvi['year'] = ndvi['index'].apply(lambda x: x[:4])\n",
    "    evi['year'] = evi['index'].apply(lambda x: x[:4])\n",
    "\n",
    "    ndvi_mean_df = ndvi.groupby(['year']).mean()\n",
    "    ndvi_max_df = ndvi.groupby(['year']).max()\n",
    "    ndvi_min_df = ndvi.groupby(['year']).min()\n",
    "\n",
    "    evi_mean_df = evi.groupby(['year']).mean()\n",
    "    evi_max_df = evi.groupby(['year']).max()\n",
    "    evi_min_df = evi.groupby(['year']).min()\n",
    "\n",
    "    # ndvi_mean_df = ndvi_mean_df.drop(columns=['index'])\n",
    "    ndvi_max_df = ndvi_max_df.drop(columns=['index'])\n",
    "    ndvi_min_df = ndvi_min_df.drop(columns=['index'])\n",
    "\n",
    "    evi_max_df = evi_max_df.drop(columns=['index'])\n",
    "    evi_min_df = evi_min_df.drop(columns=['index'])\n",
    "\n",
    "    ndvi_mean_df = ndvi_mean_df.T\n",
    "    ndvi_max_df = ndvi_max_df.T\n",
    "    ndvi_min_df = ndvi_min_df.T\n",
    "    evi_mean_df = evi_mean_df.T\n",
    "    evi_max_df = evi_max_df.T\n",
    "    evi_min_df = evi_min_df.T\n",
    "\n",
    "    return ndvi_mean_df, ndvi_max_df, ndvi_min_df, evi_mean_df, evi_max_df, evi_min_df\n",
    "\n",
    "def get_label_data() -> [pd.DataFrame, MinMaxScaler] :\n",
    "    label_path = str(Path(os.getcwd()).parent.parent.absolute()) + '/Yield_Data/all_country_crop_yield_tons_per_hectare.csv'\n",
    "    df = pd.read_csv(label_path)\n",
    "    df = df.set_index(['Country Name'])\n",
    "    df = df.iloc[:, :-3]\n",
    "    countries_list_nme = get_selected_africa_coutries_list()\n",
    "    temp2 = [i in countries_list_nme for i in df.index.tolist()]\n",
    "    df2 = df[temp2]\n",
    "    print('Data Range Before Scale:',df2.max().max(), ' to ',  df2.min().min())\n",
    "    scaler = MinMaxScaler(feature_range=(0, 2))\n",
    "    temp = df2.copy().to_numpy().reshape(-1, 1)\n",
    "    scaler = scaler.fit(temp)\n",
    "    for i in df2.columns.tolist():\n",
    "        df2.loc[:, i] = scaler.transform(df2[i].values.reshape(-1, 1))\n",
    "    print('Data Range After Scale:', df2.max().max(), ' to ', df2.min().min())\n",
    "    return df2, scaler\n",
    "\n",
    "\n",
    "def LSTM_Model(input_instance, features_size, opt):\n",
    "    n_features = 1\n",
    "    m = 32\n",
    "    LSTM_model = Sequential()\n",
    "    LSTM_model.add(LSTM(128, return_sequences=True, input_shape=(input_instance, features_size)))\n",
    "    LSTM_model.add(Dropout(0.2))\n",
    "    LSTM_model.add(LSTM(units=128, return_sequences=True, input_shape=(input_instance, features_size)))\n",
    "    LSTM_model.add(Dropout(0.2))\n",
    "    LSTM_model.add(LSTM(units=128, return_sequences=False))\n",
    "    LSTM_model.add(Dropout(0.2))\n",
    "    LSTM_model.add(Flatten())\n",
    "    LSTM_model.add(Dense(1))\n",
    "    LSTM_model.compile(optimizer=Adamax(learning_rate=0.001), loss=MeanAbsoluteError())\n",
    "    # LSTM_model.compile(optimizer=Adam(learning_rate=0.001), loss=MeanAbsoluteError())\n",
    "    # LSTM_model.compile(optimizer=Adam(learning_rate=0.001), loss=MeanSquaredError())\n",
    "    # LSTM_model.add(MDN(n_features, m))\n",
    "    # LSTM_model.compile(loss=get_mixture_loss_func(n_features, m), optimizer=Adam(lr=0.001)) # 200 Epoch 256 LSTM\n",
    "    # LSTM_model.compile(loss=get_mixture_loss_func(n_features, m), optimizer=Adamax(lr=0.0001)) # 1000 Epoch 512 LSTM\n",
    "    # LSTM_model.compile(loss=get_mixture_loss_func(n_features, m), optimizer=Adamax(lr=0.00001))  # 5000 Epoch 128 LSTM 0.22\n",
    "    # LSTM_model.compile(loss=get_mixture_loss_func(n_features, m), optimizer=Adamax(lr=0.0001))  # 600 Epoch 128 LSTM 0.26\n",
    "    # LSTM_model.compile(loss=get_mixture_loss_func(n_features, m), optimizer=Adam(lr=0.0001))  # 10000 Epoch 128 LSTM 0.24 MAPE\n",
    "    # LSTM_model.compile(loss=get_mixture_loss_func(n_features, m), optimizer=Adam(lr=0.001))  # 10000 Epoch 128 LSTM\n",
    "    # LSTM_model.compile(loss=get_mixture_loss_func(n_features, m), optimizer=opt, metrics=[metrics.MeanSquaredLogarithmicError(),\n",
    "    #                                                                                       metrics.MeanAbsolutePercentageError(),\n",
    "    #                                                                                       metrics.RootMeanSquaredError()])\n",
    "    # LSTM_model.compile(loss=get_mixture_loss_func(n_features, m), optimizer=opt)\n",
    "    return LSTM_model\n",
    "\n",
    "\n",
    "class TestCallback(Callback):\n",
    "    def __init__(self, train_data, valid_data, scaler):\n",
    "        self.valid_data = valid_data\n",
    "        self.train_data = train_data\n",
    "        self.loss_list = []\n",
    "        self.msle_list = []\n",
    "        self.rmse_list = []\n",
    "        self.mape_list = []\n",
    "        self.scaler = scaler\n",
    "        self.best_model = None\n",
    "        self.min_valid_mape = 1\n",
    "        self.min_train_pred = []\n",
    "        self.min_valid_pred = []\n",
    "        self.best_epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        x, y = self.valid_data\n",
    "        epoch_pred = self.model.predict(x)\n",
    "        # valid_pred = np.apply_along_axis(sample_from_output, 1, epoch_pred, 1, 32, temp=1.0).reshape(-1, 1)\n",
    "        valid_y = self.scaler.inverse_transform(y.reshape(-1, 1))\n",
    "        valid_pred = self.scaler.inverse_transform(epoch_pred.reshape(-1, 1))\n",
    "        valid_rmse = mean_squared_error(valid_y, valid_pred, squared=False)\n",
    "        valid_mape = abs((valid_y - valid_pred) / valid_y).mean()\n",
    "\n",
    "        self.rmse_list.append(valid_rmse)\n",
    "        self.mape_list.append(valid_mape)\n",
    "\n",
    "        # results = self.model.evaluate(x, y)\n",
    "        # print(results)\n",
    "        if self.min_valid_mape > valid_mape:\n",
    "            self.min_valid_mape = valid_mape\n",
    "\n",
    "            # Save the model\n",
    "            self.best_model = clone_model(self.model)\n",
    "            self.best_model.build((None, 10))\n",
    "            self.best_model.compile(optimizer=Adamax(learning_rate=0.001), loss=MeanAbsoluteError())\n",
    "            self.best_model.set_weights(self.model.get_weights())\n",
    "\n",
    "            train_x, train_y = self.train_data\n",
    "            # Get the prediction first in case anythings\n",
    "            self.min_train_pred = self.model.predict(train_x)\n",
    "            self.min_valid_pred = self.model.predict(x)\n",
    "\n",
    "            self.best_epoch = epoch\n",
    "\n",
    "        print(f'  Valid rmse: {valid_rmse}, mape {valid_mape}')\n",
    "\n",
    "\n",
    "def LSTM_model_prediction() -> list:\n",
    "    # data_test = normalize_test(data_test, 0, 1, scaler)\n",
    "    df_label, scaler = get_label_data()\n",
    "    opt_list = [Adam(learning_rate=0.001)\n",
    "                # , Adam(learning_rate=0.0001)\n",
    "                # , Adam(learning_rate=0.00001)\n",
    "                # , Adamax(learning_rate=0.001)\n",
    "                # , Adamax(learning_rate=0.0001)\n",
    "                # ,Adamax(learning_rate=0.00001)\n",
    "                ]\n",
    "    opt_list_str = ['Adam(lr=0.001)',\n",
    "                'Adam(lr=0.0001)',\n",
    "                'Adam(lr=0.00001)',\n",
    "                'Adamax(lr=0.001)',\n",
    "                'Adamax(lr=0.0001)',\n",
    "                'Adamax(lr=0.00001)']\n",
    "    dim_list = ['ori',\n",
    "                # (100, 10),\n",
    "                # (50, 50),\n",
    "                # (10, 100)\n",
    "                ]\n",
    "    dim_eval_object = []\n",
    "    history_list = []\n",
    "    lowest_valid_mape = []\n",
    "    epoch_list = [5000]\n",
    "\n",
    "    for dim in dim_list:\n",
    "        all_opt_eval_object = []\n",
    "        for opt in opt_list:\n",
    "            for epoch in epoch_list:\n",
    "                train_x, train_y, test_x, valid_x, valid_y, hm_train, hm_test = LSTM_data_extraction_and_batching(df_label, dim)\n",
    "                train_x = train_x.reshape(len(train_x) , len(train_x[0]), len(train_x[0][0]))\n",
    "                valid_x = valid_x.reshape(len(valid_x), len(valid_x[0]), len(valid_x[0][0]))\n",
    "                epoch_eval = TestCallback((train_x, train_y), (valid_x, valid_y), scaler)\n",
    "                LSTM_model = LSTM_Model(len(train_x[0]), len(train_x[0][0]), opt)\n",
    "                history = LSTM_model.fit(train_x, train_y, epochs=epoch, batch_size=len(train_x),\n",
    "                                         validation_data=(valid_x, valid_y),\n",
    "                                         callbacks=[epoch_eval])\n",
    "                all_opt_eval_object.append(epoch_eval)\n",
    "                history_list.append(history)\n",
    "        # epoch_eval.best_model.save('D:\\Saved_LSTM_model\\Adam_0_001')\n",
    "        print('Finished Training')\n",
    "\n",
    "        # plt.figure(figsize=(10, 6), dpi=72)\n",
    "        # for i in range(len(all_opt_eval_object)):\n",
    "        #     eval_obj = all_opt_eval_object[i]\n",
    "        #     plt.plot(range(len(eval_obj.rmse_list)), eval_obj.rmse_list , label=opt_list_str[i])\n",
    "        # plt.legend()\n",
    "        # plt.title(\"RMSE comparison\")\n",
    "        # plt.show()\n",
    "        #\n",
    "        # plt.figure(figsize=(10, 6), dpi=72)\n",
    "        # for i in range(len(all_opt_eval_object)):\n",
    "        #     eval_obj = all_opt_eval_object[i]\n",
    "        #     plt.plot(range(len(eval_obj.mape_list)), eval_obj.mape_list, label=opt_list_str[i])\n",
    "        # plt.legend()\n",
    "        # plt.title(\"MAPE comparison\")\n",
    "        # plt.show()\n",
    "        min_mape = 1\n",
    "        for i in all_opt_eval_object:\n",
    "            if min_mape > min(i.mape_list):\n",
    "                min_mape = min(i.mape_list)\n",
    "        lowest_valid_mape.append(min_mape)\n",
    "\n",
    "        dim_eval_object.append(all_opt_eval_object)\n",
    "\n",
    "    print('Min MAPE for all three shape', lowest_valid_mape)\n",
    "    LSTM_pred = []\n",
    "\n",
    "    return LSTM_pred\n",
    "\n",
    "def hybrid_prediction():\n",
    "    params_setting = {'bootstrap': [True, False],\n",
    "                     'max_depth': [2, 3, 4, 5, 6, 7, None],\n",
    "                     'max_features': ['auto', 'sqrt'],\n",
    "                     'min_samples_leaf': [1, 2, 4],\n",
    "                     'min_samples_split': [2, 5, 10],\n",
    "                     'n_estimators': [50, 150, 200, 250, 300, 400, 500, 600, 1000, 1200, 1400, 1600, 1800, 2000]}\n",
    "\n",
    "    df_label, scaler = get_label_data()\n",
    "    train_x, train_y, test_x, valid_x, valid_y, hm_train_x, hm_valid_x = LSTM_data_extraction_and_batching(df_label, 'ori')\n",
    "\n",
    "    df_scaler = MinMaxScaler(feature_range=(0,4)).fit(hm_train_x.iloc[:, :])\n",
    "    hm_train_x.iloc[:, :] = df_scaler.transform(hm_train_x.iloc[:, :])\n",
    "    hm_valid_x.iloc[:, :] = df_scaler.transform(hm_valid_x.iloc[:, :])\n",
    "\n",
    "\n",
    "    model = keras.models.load_model('D:\\Saved_LSTM_model\\model1')\n",
    "\n",
    "\n",
    "    train_pred = scaler.inverse_transform(model.predict(train_x).reshape(-1,1))\n",
    "    valid_pred = scaler.inverse_transform(model.predict(valid_x).reshape(-1,1))\n",
    "\n",
    "    hm_train_x['lstm_pred'] = train_pred\n",
    "    hm_valid_x['lstm_pred'] = valid_pred\n",
    "\n",
    "\n",
    "    hm_train_x['label'] = scaler.inverse_transform(train_y.reshape(-1, 1))\n",
    "    hm_valid_x['label'] = scaler.inverse_transform(valid_y.reshape(-1, 1))\n",
    "\n",
    "    predictor = TabularPredictor(label='label', problem_type='regression').fit(hm_train_x, presets=['best_quality'], num_stack_levels=3)\n",
    "    auto_pred = predictor.predict(hm_valid_x)\n",
    "    print('LSTM + AUTOML, MAPE:', abs((valid_y - auto_pred) / valid_y).mean())\n",
    "\n",
    "    # gs_rf = GridSearchCV(estimator= RandomForestRegressor(), param_grid=params_setting,cv=5, n_jobs=-1, verbose=2)\n",
    "    # gs_rf.fit(hm_train_x, train_y)\n",
    "\n",
    "\n",
    "#     valid_pred = gs_rf.predict(hm_valid_x)\n",
    "    valid_y = scaler.inverse_transform(valid_y.reshape(-1, 1))\n",
    "#     valid_pred = scaler.inverse_transform(valid_pred.reshape(-1, 1))\n",
    "    valid_rmse = mean_squared_error(valid_y, auto_pred, squared=False)\n",
    "    valid_mape = abs((valid_y - auto_pred) / valid_y).mean()\n",
    "\n",
    "    print('Final RMSE:', valid_rmse)\n",
    "    print('Final MAPE:', valid_mape)\n",
    "\n",
    "\n",
    "\n",
    "    return\n",
    "\n",
    "def LSTM_data_extraction_and_batching(df_label:pd.DataFrame, resize_dim) -> [list, list, list, list, list, pd.DataFrame, pd.DataFrame]:\n",
    "    processed_path = Path(str(Path(os.getcwd()).parent.parent.absolute()) + '/ProcessedHistograms')\n",
    "    country_list = sorted(processed_path.glob('*'))\n",
    "    train_x, train_y, test_x, valid_x, valid_y = [], [], [], [], []\n",
    "    water_train_x, water_test_x = [], []\n",
    "    ndvi_mean_train_x, ndvi_mean_test_x = [], []\n",
    "    ndvi_max_train_x, ndvi_max_test_x = [], []\n",
    "    ndvi_min_train_x, ndvi_min_test_x = [], []\n",
    "    evi_mean_train_x, evi_mean_test_x = [], []\n",
    "    evi_max_train_x, evi_max_test_x = [], []\n",
    "    evi_min_train_x, evi_min_test_x = [], []\n",
    "\n",
    "    counter, sum,counter_new, sum_new= 0, 0, 0, 0\n",
    "\n",
    "    water_data = get_water_data()\n",
    "    ndvi, evi =  get_ndvi_evi_data()\n",
    "    ndvi_mean_df, ndvi_max_df, ndvi_min_df, evi_mean_df, evi_max_df, evi_min_df = ndvi_evi_monthly_to_yearly(ndvi, evi)\n",
    "\n",
    "    scaler_data = None\n",
    "    for country in country_list:\n",
    "        if '.' in str(country):\n",
    "            continue\n",
    "        data_list = sorted(country.glob('*.npy'))\n",
    "        country_nme = str(country).split('\\\\')[-1]\n",
    "        country_nme = country_nme.replace('_', ' ')\n",
    "\n",
    "        try:\n",
    "            country_label = df_label.loc[country_nme]\n",
    "            water_label = water_data.loc[country_nme]\n",
    "            ndvi_mean_label = ndvi_mean_df.loc[country_nme]\n",
    "            ndvi_max_label = ndvi_max_df.loc[country_nme]\n",
    "            ndvi_min_label = ndvi_min_df.loc[country_nme]\n",
    "            evi_mean_label = evi_mean_df.loc[country_nme]\n",
    "            evi_max_label = evi_max_df.loc[country_nme]\n",
    "            evi_min_label = evi_min_df.loc[country_nme]\n",
    "\n",
    "            for i in data_list:\n",
    "                year = str(i).split('_')[-1][:4]\n",
    "                data_array = np.load(str(i.absolute()))\n",
    "                for j in data_array:\n",
    "                    counter += Counter(j)[0]\n",
    "                    sum += len(j)\n",
    "                if scaler_data is None:\n",
    "                    scaler_data = data_array\n",
    "                data = np.transpose(data_array)\n",
    "                if resize_dim != 'ori':\n",
    "                    data = im.fromarray(data).resize(resize_dim)\n",
    "                    data = np.array(data)\n",
    "                for j in data:\n",
    "                    counter_new += Counter(j)[0]\n",
    "                    sum_new += len(j)\n",
    "                if year < '2015':\n",
    "                    train_x.append(data)\n",
    "                    train_y.append(country_label[year])\n",
    "\n",
    "                    water_train_x.append(water_label[year])\n",
    "                    ndvi_mean_train_x.append(ndvi_mean_label[year])\n",
    "                    ndvi_max_train_x.append(ndvi_max_label[year])\n",
    "                    ndvi_min_train_x.append(ndvi_min_label[year])\n",
    "                    evi_mean_train_x.append(evi_mean_label[year])\n",
    "                    evi_max_train_x.append(evi_max_label[year])\n",
    "                    evi_min_train_x.append(evi_min_label[year])\n",
    "\n",
    "                elif year < '2019':\n",
    "                    valid_x.append(data)\n",
    "                    valid_y.append(country_label[year])\n",
    "\n",
    "                    water_test_x.append(water_label[year])\n",
    "                    ndvi_mean_test_x.append(ndvi_mean_label[year])\n",
    "                    ndvi_max_test_x.append(ndvi_max_label[year])\n",
    "                    ndvi_min_test_x.append(ndvi_min_label[year])\n",
    "                    evi_mean_test_x.append(evi_mean_label[year])\n",
    "                    evi_max_test_x.append(evi_max_label[year])\n",
    "                    evi_min_test_x.append(evi_min_label[year])\n",
    "                else:\n",
    "                    test_x.append(data)\n",
    "        except KeyError:\n",
    "            print(country_nme, 'label dataset is missing.', 'Passed ', country_nme, 'data.')\n",
    "    print('-'*50)\n",
    "    print('Training dataset sizes:', len(train_x))\n",
    "    print('Validation dataset sizes:', len(valid_x))\n",
    "    print('Testing dataset sizes:', len(test_x))\n",
    "    # print('Number of 0:', counter)\n",
    "    # print('Number of sum element:', sum)\n",
    "    # print('Percentage of 0:', counter/sum)\n",
    "    # print('Number of 0 new:', counter_new)\n",
    "    # print('Number of sum element new:', sum_new)\n",
    "    # print('Percentage of 0 new:', counter_new/sum_new)\n",
    "\n",
    "    hybrid_model_train_data = pd.DataFrame(data={'water':water_train_x,\n",
    "                                                 'ndvi_mean':ndvi_mean_train_x,\n",
    "                                                 'ndvi_max':ndvi_max_train_x,\n",
    "                                                 'ndvi_min':ndvi_min_train_x,\n",
    "                                                 'evi_mean':evi_mean_train_x,\n",
    "                                                 'evi_max':evi_max_train_x,\n",
    "                                                 'evi_min':evi_min_train_x\n",
    "                                                 })\n",
    "\n",
    "    hybrid_model_test_data = pd.DataFrame(data={'water':water_test_x,\n",
    "                                                 'ndvi_mean':ndvi_mean_test_x,\n",
    "                                                 'ndvi_max':ndvi_max_test_x,\n",
    "                                                 'ndvi_min':ndvi_min_test_x,\n",
    "                                                 'evi_mean':evi_mean_test_x,\n",
    "                                                 'evi_max':evi_max_test_x,\n",
    "                                                 'evi_min':evi_min_test_x\n",
    "                                                 })\n",
    "\n",
    "    return np.array(train_x), np.array(train_y), np.array(test_x), np.array(valid_x), np.array(valid_y), hybrid_model_train_data, hybrid_model_test_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158eb6f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# df_label, scaler = get_label_data()\n",
    "# temp = get_water_data()\n",
    "# train_x, train_y, test_x, valid_x, valid_y, hm_train_x, hm_test_x = LSTM_data_extraction_and_batching(df_label, 'ori')\n",
    "# LSTM_pred = LSTM_model_prediction()\n",
    "# print('Prediction:', LSTM_pred)\n",
    "hybrid_prediction()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2779cb9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:MSBD5001_INDI]",
   "language": "python",
   "name": "conda-env-MSBD5001_INDI-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
